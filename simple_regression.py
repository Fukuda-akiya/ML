import numpy as np
import matplotlib.pyplot as plt
import show_graph as show
from numpy.polynomial import Polynomial
from sklearn.linear_model import LinearRegression
"""
def residual(D, x, y):
    A = [] * len(D)
    for i in range(len(D)):
	A[i] = 
"""

X = np.array([
     9.1, 11.2, 12.3, 18.9, 22.2, 26. , 30.9, 31.2, 28.8, 23. , 18.3,
         11.1,  8.3,  9.1, 12.5, 18.5, 23.6, 24.8, 30.1, 33.1, 29.8, 23. ,
	     16.3, 11.2,  9.6, 10.3, 16.4, 19.2, 24.1, 26.5, 31.4, 33.2, 28.8,
	         23. , 17.4, 12.1, 10.6,  9.8, 14.5, 19.6, 24.7, 26.9, 30.5, 31.2,
		     26.9, 23. , 17.4, 11. , 10.4, 10.4, 15.5, 19.3, 26.4, 26.4, 30.1,
		         30.5, 26.4, 22.7, 17.8, 13.4, 10.6, 12.2, 14.9, 20.3, 25.2, 26.3,
			     29.7, 31.6, 27.7, 22.6, 15.5, 13.8, 10.8, 12.1, 13.4, 19.9, 25.1,
			         26.4, 31.8, 30.4, 26.8, 20.1, 16.6, 11.1,  9.4, 10.1, 16.9, 22.1,
				     24.6, 26.6, 32.7, 32.5, 26.6, 23. , 17.7, 12.1, 10.3, 11.6, 15.4,
				         19. , 25.3, 25.8, 27.5, 32.8, 29.4, 23.3, 17.7, 12.6, 11.1, 13.3,
					     16. , 18.2, 24. , 27.5, 27.7, 34.1, 28.1, 21.4, 18.6, 12.3])

Y = np.array([
    463.,  360.,  380.,  584.,  763.,  886., 1168., 1325.,  847.,
        542.,  441.,  499.,  363.,  327.,  414.,  545.,  726.,  847.,
	   1122., 1355.,  916.,  571.,  377.,  465.,  377.,  362.,  518.,
	       683.,  838., 1012., 1267., 1464., 1000.,  629.,  448.,  466.,
	           404.,  343.,  493.,  575.,  921., 1019., 1149., 1303.,  805.,
		       739.,  587.,  561.,  486.,  470.,  564.,  609.,  899.,  946.,
		          1295., 1325.,  760.,  667.,  564.,  633.,  478.,  450.,  567.,
			      611.,  947.,  962., 1309., 1307.,  930.,  668.,  496.,  650.,
			          506.,  423.,  531.,  672.,  871.,  986., 1368., 1319.,  924.,
				      716.,  651.,  708.,  609.,  535.,  717.,  890., 1054., 1077.,
				         1425., 1378.,  900.,  725.,  554.,  542.,  561.,  459.,  604.,
					     745., 1105.,  973., 1263., 1533., 1044.,  821.,  621.,  601.,
					         549.,  572.,  711.,  819., 1141., 1350., 1285., 1643., 1133.,
						     784.,  682.,  587.])
# データの格納
icecream_data = (X, Y)

# 線形回帰モデルのパラメータを求める
W = Polynomial.fit(X, Y, 1) 		# 単回帰の場合1を指定
W_a_and_b = W.convert().coef 		# 返り値Wから切片bと傾きaを求める
x = np.linspace(0, 35, 100)		# 0-35を100等分する
y_hat = W(x)				# 0-35の100個の数値xに対して、目的変数の推定値y_hatを求める

# skleranによる線形回帰
reg = LinearRegression()
reg.fit(X.reshape(-1,1), Y)		# 事例数を行数、目的変数の数を列数とした行列で与える（-1は行ベクトルを与える）
print(reg.coef_)			# 回帰直線の傾き
print(reg.intercept_)			# 回帰直線の切片
print(reg.score(X.reshape(-1,1), Y))	# 決定係数を求める
x = np.linspace(0, 35, 100)             # 0-35を100等分する
y_hat = reg.predict(x.reshape(-1,1))	# predict関数で回帰直線を求める

# 確認問題
D = np.array([[1, 3], [3, 6], [6, 5], [8, 7]])	# データ
X = D[:,0]					# 0列目の値を代入
Y = D[:,1]					# 1列目の値を代入
reg = LinearRegression()
reg.fit(X.reshape(-1, 1), Y)
print(reg.coef_)
print(reg.intercept_)
print(reg.score(X.reshape(-1, 1), Y))
x = np.linspace(0, 10, 2)
y_hat = reg.predict(x.reshape(-1,1))

# グラフ描画関数の呼び出し
show.show_graph(X, Y, x, y_hat)
